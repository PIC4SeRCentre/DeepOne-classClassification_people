{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmhG0pdpbKwt"
   },
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xPn94WQkxH32",
    "outputId": "b7b760d9-3e3c-4166-bc37-f4587dff9679"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from six.moves import urllib\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZHXfZ5IYK8g"
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "preprocess_input = applications.mobilenet_v2.preprocess_input \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run models on GPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[1], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set useful paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder structure is the following: there is a main folder *Dataset* that contains all sub folders where taking picture to pre-process (*data_test*, *data_test2_esterni* and *test_surveillance*) and where putting all pre-processed ones (*testBN*, *test2BN_esterni* and *testIR*).\n",
    "\n",
    "The class we want to recognize among all is still the *target class Persona*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds = \"Dataset\"\n",
    "target = \"Persona\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgWpq37tsu1i"
   },
   "source": [
    "## Losses computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two customized losses must be defined to import correctly the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantities *batch_size* and *sub_batch_size* are defined, with also the constant *beta* which is used in the *compactness loss* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Cu1ytXPl-RXL",
    "outputId": "26cd2b9b-9aa7-423d-e971-5f8109595988"
   },
   "outputs": [],
   "source": [
    "#batch_size = 256   \n",
    "batch_size = 32\n",
    "sub_batch_size = batch_size // 2\n",
    "beta = sub_batch_size**2 / (sub_batch_size-1)**2  #1.0158 with batch_size=256\n",
    "print(\"beta = \", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two input quantities of *compactness loss* are:\n",
    "\n",
    "• *y_true*: the true labels of the batch, of size (batch_size, n_classes_ref).\n",
    "This quantity is not used in the lc computation because it has no role in\n",
    "imposing similarity among person features;\n",
    "\n",
    "• *y_pred*: predictions of the intermediate features for each element in the\n",
    "batch, of size (batch_size, n_features). It is produced by the average pooling layer, so the number of features is 1280. We choose this layer because it has weights pre-trained on ImageNet, that speed up the learning process compared to those with random inizialization.\n",
    "\n",
    "In order to consider only features of person images, the first half part of the batch is isolated. Then, the following operations are performed: the variance of the feature distribution along the batch for each feature and the mean of all variances. This number is then multiplied by a correction factor beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VoPxt-FE2Q2U"
   },
   "outputs": [],
   "source": [
    "def compactness_loss(y_true, y_pred):\n",
    "    #y_pred_target = y_pred[0:128]   #shape (128, 1280)\n",
    "    y_pred_target = y_pred[0:16]   #shape (16, 1280)\n",
    "    # ERRATA -> l_c = tf.keras.backend.mean(tf.keras.backend.var(y_pred_target, axis = 1, keepdims=False)) \n",
    "    #axis = 1 means variance along the row -> tf.keras.backend.var of shape=(128,)\n",
    "    l_c = tf.keras.backend.mean(tf.keras.backend.var(y_pred_target, axis = 0, keepdims=False)) \n",
    "    #axis = 0 means variance along the columns (so the features)-> tf.keras.backend.var of shape=(2048,)\n",
    "\n",
    "    return l_c * beta\n",
    "\n",
    "#when features are extraxted from convolutional layer: apply average pooling layer ->  compute loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *descriptiveness loss* is computed using the *cross-entropy loss*, that is here defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tQXtD10Zcpv"
   },
   "outputs": [],
   "source": [
    "#Categorical crossentropy loss used in the descriptiveness loss\n",
    "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False) \n",
    "\n",
    "#**Note - Using from_logits=True is more numerically stable.** -> remove softmax layer\n",
    "#used default redution: reduction=losses_utils.ReductionV2.AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two input quantities of *descriptiveness loss* are:\n",
    "\n",
    "• *y_true*: the true labels of the batch, of size (batch_size, n_classes_ref).\n",
    "This quantity is provided by the inputgenerator, later defined;\n",
    "\n",
    "• *y_pred*: predictions coming from the last fully connected layer, of size\n",
    "(batch_size, n_classes_ref). The second dimension n_classes_ref is 20,\n",
    "corresponding to the categorical label of classes from the reference dataset.\n",
    "The label of the person class is not included because this is not a multiclass classification problem.\n",
    "\n",
    "The descriptiveness loss is computed with respect to only elements of the reference dataset. Therefore, the second half part of the batch is considered both in *y_true* and in *y_pred*. The first part of them contains meaningless numbers, because we don’t care about person image labels.\n",
    "Then, the categorical cross-entropy loss is evaluated between the predicted labels and the desired ones and it is minimized to realize a good classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKNJj57E2Ym6"
   },
   "outputs": [],
   "source": [
    "def descriptiveness_loss(y_true, y_pred): \n",
    "    #y_true_reference = y_true[128:256]  #shape (128, 20)\n",
    "    #y_pred_reference = y_pred[128:256]  #shape (128, 20)\n",
    "    y_true_reference = y_true[16:32]  #shape (16, 20)\n",
    "    y_pred_reference = y_pred[16:32]  #shape (16, 20)\n",
    "    l_d = cce(y_true_reference, y_pred_reference)\n",
    "    return l_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rla8h3dbmIuf"
   },
   "source": [
    "## Load trained model and isolate *model_features* for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWbHYVw7mWq_"
   },
   "outputs": [],
   "source": [
    "path_model = os.path.join(path_ds, \"my_model200_400.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1Tf4dzRviE5"
   },
   "outputs": [],
   "source": [
    "model_tot = load_model(path_model, custom_objects={'compactness_loss': compactness_loss, 'descriptiveness_loss': descriptiveness_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize properties of all layers that are part of the *model_tot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DK5_yMplxmzp"
   },
   "outputs": [],
   "source": [
    "model_tot.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSmQASU3mm3b"
   },
   "source": [
    "From *model_tot* we extrapolate the model for feature extraction: *model_features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZzaJBQ0qXz6"
   },
   "outputs": [],
   "source": [
    "model_features = Model(model_tot.inputs, model_tot.layers[-2].output) #output = <tf.Tensor 'dense_2/Identity:0' shape=(None, 1280) dtype=float32>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize properties of all layers from the *model_features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIEbjFmJlPFa"
   },
   "outputs": [],
   "source": [
    "model_features.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPfwNrn45-8Q"
   },
   "source": [
    "# Testing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing part is realized by a *template matching framework*: firstly, in the *template generation phase*, some baseline features of person intances are stored as templates and then, in *matching phase*, a score is generated considering the Euclidean distance between them and new features from the test image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In template generation phase, some samples are selected from the target dataset and are sent into *model_features* to generate *templates*.\n",
    "\n",
    "Firstly, templates are taken from folder *templates*, transformed into 224x224 grayscale images and saved in folder *templatesBN*. Templates for grayscale datasets BN1 and BN2 are 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xD2ML82MEFfo"
   },
   "outputs": [],
   "source": [
    "path_images_templates = os.path.join(path_ds, \"templates\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images_templatesBN = os.path.join(path_ds, \"templatesBN\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "for folder in os.listdir(path_images_templates):\n",
    "    path_folder = os.path.join(path_images_templates, folder)\n",
    "    print(\"\\n------------------------------------------------------\")\n",
    "    print(\"\\nFolder \", folder, \" with \", len(os.listdir(path_folder)), \"images inside\")\n",
    "\n",
    "    path_out = os.path.join(path_images_templatesBN, \"templates_Persona\")\n",
    "    if not os.path.exists(path_out):\n",
    "        os.makedirs(path_out)\n",
    "\n",
    "    i=0   #new images\n",
    "    j=0   #images already pre-processed\n",
    "    for file in os.listdir(path_folder):\n",
    "        if os.path.exists(path_out + \"/\" + file):\n",
    "            j+=1\n",
    "            print(\"Image \" + file + \" already pre-processed\" )\n",
    "        else:\n",
    "            i+=1\n",
    "            print(\"Processing ... \", file)\n",
    "            \n",
    "            #read the image\n",
    "            image = cv2.imread(path_folder + \"/\"+ file)\n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)     #gray image\n",
    "            image = cv2.merge((image, image, image))            #gray image on 3 channels\n",
    "            #write the pre-proc image in train folder\n",
    "            cv2.imwrite(path_out + \"/\" + file, image)\n",
    "\n",
    "    print(\"\\nImages that have been previously pre-processed: \" + str(j))\n",
    "    print(\"\\nNewly pre-processed images: \" + str(i))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then *templates* of size (40, 1280) are produced by *test_datagen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(path_images_templatesBN,\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  shuffle = False,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  batch_size= len(os.listdir(os.path.join(path_images_templatesBN, \"templates_Persona\"))))  #50\n",
    "\n",
    "templates = model_features.predict(test_generator,steps = 1)  #(n_templates, n_features) ex. (40, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "batch = next(test_generator)\n",
    "for i in range (0,10):\n",
    "    image = batch[0][i]\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates.shape  #(40, 1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0dRXypAMxo2"
   },
   "source": [
    "## 1.1 Creation of  predictions *features_test* and true labels *Y_test* (in DOC labels are 0: target class = Person, 1: alien class, no people inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we extract features from test images. The two grayscale dataset contain both 1000 pictures with people and 1000 pictures without individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbH9xAyMmX2l"
   },
   "source": [
    "## Pre-process test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtUXIQB4gTbL"
   },
   "source": [
    "Folder *data_test* and *data_test2_esterni* contains RGB images of the two categories to pre-process e to store respectively in folders *test* and *test2BN_esterni*.\n",
    "\n",
    "The structure of folders is the following:\n",
    "<pre>\n",
    "<b>data_test or data_test2_esterni</b>\n",
    "|__ <b>Persona</b>\n",
    "|__ <b>Others</b>\n",
    "</pre>\n",
    "\n",
    "<pre>\n",
    "<b>testBN or test2BN_esterni</b>\n",
    "|__ <b>Persona</b>\n",
    "   |__ <b>1</b>\n",
    "|__ <b>Others</b>\n",
    "   |__ <b>0</b>\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set useful paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_test = os.path.join(path_ds, \"data_test\") \n",
    "path_test = os.path.join(path_ds, \"testBN\")\n",
    "\n",
    "#path_data_test = os.path.join(path_ds, \"data_test2_esterni\")\n",
    "#path_test = os.path.join(path_ds, \"test2BN_esterni\")\n",
    "\n",
    "path_test_Persona = os.path.join(path_test, \"Persona\")\n",
    "\n",
    "path_test_Others = os.path.join(path_test, \"Others\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbH9xAyMmX2l"
   },
   "source": [
    "Pre-processing:\n",
    "\n",
    "• each image is centrally cropped along its smaller size. In this way we\n",
    "can resize it without altering the image aspect ratio and the properties\n",
    "of objects within;\n",
    "\n",
    "• each picture is resized to square format of 224×224 with a bilinear interpolation;\n",
    "\n",
    "• each image is made a grayscale image with size of (224, 224, 1), having a\n",
    "single channel;\n",
    "\n",
    "• each grayscale image is brought back on three channels, repeating the single channel three times. This operation is done since the structure of\n",
    "most of networks presents a three channel configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "for folder in os.listdir(path_data_test):\n",
    "    path_folder = os.path.join(path_data_test, folder)\n",
    "    print(\"\\n------------------------------------------------------\")\n",
    "    print(\"\\nFolder \", folder, \" with \", len(os.listdir(path_folder)), \"images inside\")\n",
    "\n",
    "    if folder == target:\n",
    "        path_out = os.path.join(path_test_Persona, \"0\")\n",
    "        if not os.path.exists(path_out):\n",
    "            os.makedirs(path_out)\n",
    "    else: \n",
    "        path_out = os.path.join(path_test_Others, \"1\")\n",
    "        if not os.path.exists(path_out):\n",
    "            os.makedirs(path_out)\n",
    "\n",
    "    i=0   #new images\n",
    "    j=0   #images already pre-processed\n",
    "    for file in os.listdir(path_folder):\n",
    "        if os.path.exists(path_out + \"/\" + file):\n",
    "            j+=1\n",
    "            print(\"Image \" + file + \" already pre-processed\" )\n",
    "        else:\n",
    "            i+=1\n",
    "            print(\"Processing ... \", file)\n",
    "            \n",
    "            #read the image\n",
    "            image = cv2.imread(path_folder + \"/\"+ file)\n",
    "            #crop image -> square image along its min dimension\n",
    "            h, w, c = image.shape\n",
    "            if w>h:\n",
    "                start = (w-h)//2\n",
    "                image = image[:, start:start+h]\n",
    "            else:\n",
    "                start = (h-w)//2\n",
    "                image = image[start:start+w,:]\n",
    "            #resize\n",
    "            image = cv2.resize(image, (img_size, img_size), interpolation=cv2.INTER_LINEAR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)     #gray image\n",
    "            image = cv2.merge((image, image, image))            #gray image on 3 channels\n",
    "            #write the pre-proc image in train folder\n",
    "            cv2.imwrite(path_out + \"/\" + file, image)\n",
    "\n",
    "    print(\"\\nImages that have been previously pre-processed: \" + str(j))\n",
    "    print(\"\\nNewly pre-processed images: \" + str(i))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from Person images using *test_datagen0*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen0 = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "test_generator0 = test_datagen0.flow_from_directory(path_test_Persona,\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  shuffle = False,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  batch_size= 100\n",
    "                                                  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Persona = model_features.predict(test_generator0, steps = len(os.listdir(os.path.join(path_test_Persona, \"0\"))) // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Persona.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from Others images using *test_datagen1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen1 = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "test_generator1 = test_datagen1.flow_from_directory(path_test_Others,\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  shuffle = False,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  batch_size= 100\n",
    "                                                  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JiBRZZYujwzy"
   },
   "outputs": [],
   "source": [
    "features_Others = model_features.predict(test_generator1, steps = len(os.listdir(os.path.join(path_test_Others, \"1\"))) // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Others.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append all features in *features_test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = np.concatenate([features_Persona, features_Others])    #Y_test will be 00000 ... 11111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a *features_test_tsne* for visualizing features and templates thanks to t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_tsne = np.concatenate([features_Persona, features_Others, templates]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create true labels of test images in *Y_test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=np.concatenate([np.zeros(features_Persona.shape[0]), np.ones(features_Others.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a *Y_test_tsne* with inside true labels of features and templates for t-SNE visualization of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_tsne=np.concatenate([np.zeros(features_Persona.shape[0]), np.ones(features_Others.shape[0]), np.ones(templates.shape[0])*2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement t-SNE visualization of 1280 features extracted from each test image.\n",
    "\n",
    "• red points with labels 0 are the features associated to images containg people;\n",
    "\n",
    "• green points labeled with 1 are the features extracted from pictures with no people;\n",
    "\n",
    "• blue points with a fake label 2 are the templates from which the classification score is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "#feat_cols = [ 'pixel'+str(i) for i in range(features_test.shape[1]) ]\n",
    "#df = pd.DataFrame(features_test,columns=feat_cols)\n",
    "#df['y'] = Y_test\n",
    "\n",
    "feat_cols = [ 'pixel'+str(i) for i in range(features_test_tsne.shape[1]) ]\n",
    "df = pd.DataFrame(features_test_tsne,columns=feat_cols)\n",
    "df['y'] = Y_test_tsne\n",
    "\n",
    "df['label'] = df['y'].apply(lambda i: str(i))\n",
    "#features_test, Y_test = None, None\n",
    "print('Size of the dataframe: {}'.format(df.shape))\n",
    "\n",
    "# For reproducability of the results\n",
    "np.random.seed(20)\n",
    "rndperm = np.random.permutation(df.shape[0])\n",
    "\n",
    "N = Y_test.shape[0]\n",
    "df_subset = df.loc[rndperm[:N],:].copy()\n",
    "data_subset = df_subset[feat_cols].values\n",
    "#pca = PCA(n_components=3)\n",
    "#pca_result = pca.fit_transform(data_subset)\n",
    "#df_subset['pca-one'] = pca_result[:,0]\n",
    "#df_subset['pca-two'] = pca_result[:,1] \n",
    "#df_subset['pca-three'] = pca_result[:,2]\n",
    "#print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "#_____________________\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(data_subset)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 3),\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xEbCv2Blexs"
   },
   "source": [
    "## 2. Template matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In matching phase, features extracted from test images are compared to templates using a *matching function f*, the Euclidean distance.\n",
    "\n",
    "In particular the inputs provided to *scores_generation* function are:\n",
    "\n",
    "• features_test: features extracted from test images, of size (n_test, n_features);\n",
    "\n",
    "• templates: stored templates corresponding to baseline characteristics of\n",
    "the person class, of size (n_templates, n_features).\n",
    "\n",
    "Features coming from each test image are compared to all templates: the quantity *d* contains the euclidean distances between them, sizing (n_templates,). Each vector *d* is computed for all images in the test dataset and is saved in the *distances_vector*, of size (n_test, n_templates). The scores, stored in the vector *scores* of size (n_test,), are selected taking the minimum value among all computed distances in distances_vector, for all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pnHMnmWXjV3j"
   },
   "outputs": [],
   "source": [
    "#def euclidean(v1, v2):\n",
    "#  return sum((p-q)**2 for p, q in zip(v1, v2)) ** .5\n",
    "\n",
    "#d = [euclidean(f, t) for t in templates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlCy6Dejhgmd"
   },
   "outputs": [],
   "source": [
    "def scores_generation(features_test, templates):\n",
    "    for f in features_test:\n",
    "        d = [np.linalg.norm(f-t) for t in templates]  #np.linalg.norm(f-t) = Euclidean norm \n",
    "        distances_vector.append(d)                    #shape of (n_X_test, n_templates): euclidean norm between each element of features_X_test and each template  \n",
    "    scores = np.amin(distances_vector, axis=1)      #axis=1 -> min value for each row\n",
    "    #scores = np.mean(distances_vector, axis=1)\n",
    "    scores = np.array(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YI5I5BrZLmVF"
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "distances_vector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9bKa_7P_hpc"
   },
   "outputs": [],
   "source": [
    "scores = scores_generation(features_test, templates)  #shape of (n_X_test,) for each image belonging to X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y1TUelI5_zla",
    "outputId": "fd933994-ce70-41ef-fd8b-59c64052a98b"
   },
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhrW83kL0tYK"
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIfSscA3tiru"
   },
   "source": [
    "## Plot ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic curve plots the True Positive Rate (TPR) versus the False Positive Rate (FPR) for all possible thresholds. It is used to evaluate DOC models.\n",
    "Best ones have ROC curves very close to the top left corner of\n",
    "the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_V2Qoj8Ttqm0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "#from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrs7-T5duqta"
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.legend()\n",
    "    plt.title('ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tdE9aA5Fn4X7",
    "outputId": "2db4b15f-c642-4ece-c1f6-6e6072e7992f"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(Y_test, scores)\n",
    "AUC = roc_auc_score(Y_test, scores)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oqqifS7e1cJ6",
    "outputId": "b657e425-7949-4a9c-8c4f-9171f8f8dbc3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr, label='DeepOneClassification(AUC = %.2f)'%AUC)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save fpr and tpr in folder *metrics* to retrieve them for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metrics = os.path.join(path_ds, \"metrics\")\n",
    "\n",
    "path_fpr = os.path.join(path_metrics, \"fpr200t.npy\") \n",
    "path_tpr = os.path.join(path_metrics, \"tpr200t.npy\")\n",
    "np.save(path_fpr, fpr, allow_pickle=True, fix_imports=True)\n",
    "np.save(path_tpr, tpr, allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute variance of Person features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the quantity minimized in the compactness loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Persona = tf.convert_to_tensor(features_Persona, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.var(features_Persona, axis = 0, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var = tf.keras.backend.mean(tf.keras.backend.var(features_Persona, axis = 0, keepdims=False))\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal threshold and DOC output *y_pred*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores are finally transformed in considerable output for One-Class Classification thanks to a threshold delta. Remember that labels in DOC are 0: person and 1:others, so the class person is the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen $\\delta$ in our Deep One-class Classification is the one that maximizes the quantity (TNR-FNR), producing an high value of TNR, the True Negative Rate, and a low value of FNR, the False Negative Rate.\n",
    "The first one indicates the ratio of negative instances correctly classified as negative, while the second one is the ratio of positive instance incorrectly classified.\n",
    "Therefore, maximizing the term (TNR-FNR) allows to reach an high value of intances classified as people that are actually people and a low value of alien objects wrongly classified as people.\n",
    "Considering also that TNR=1-FPR and FNR=1-TPR, finding the maximum value for (TNR-FNR) means maximizing (TPR-FPR) ->TNR-FNR=1-FPR-(1-TPR)=TPR-FPR, that corresponds to the closest point to the top left corner of the ROC curve. These quantities have been already computed by *roc_curve* command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Threshold value is:\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros(scores.shape[0])\n",
    "for i in range(scores.shape[0]):\n",
    "    if scores[i] > optimal_threshold:\n",
    "        y_pred[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are \"reverse\" in DOC (0: person, 1:others) w.r.t. binary models. \n",
    "\n",
    "The fact that the positive class is not the *person class* causes issues because metrics are closely related to the chosen positive class.\n",
    "\n",
    "If we want to refer all metrics that are precision, recall, F1 score, accuracy to the target class, we need to reverse label values produced by DOC models -> 1-Y_test and 1-y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(1-Y_test, 1-y_pred)\n",
    "print(cm)\n",
    "#           predicted               0:negative - others   1:positive - person\n",
    "           #   0  1                   FP=false positive, actual others but predicted person\n",
    "#actual    #0 TN FP                   FN=false negative, actual person but predicted others\n",
    "           #1 FN TP                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "#f1_score(Y_test, y_pred)\n",
    "f1_score(1-Y_test, 1-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "#precision_score(Y_test, y_pred)\n",
    "precision_score(1-Y_test, 1-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "#recall_score(Y_test, y_pred)\n",
    "recall_score(1-Y_test, 1-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "#accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "# or simply\n",
    "accuracy_score(1-Y_test, 1-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot DET curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Detection Error Tradeoff curve plots the False Positive Rate (FPR) against the False Negative Rate (FNR) for all possible threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps=fpr\n",
    "fns=1-tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def DETCurve(fps,fns):\n",
    "    \"\"\"\n",
    "    Given false positive and false negative rates, produce a DET Curve.\n",
    "    The false positive rate is assumed to be increasing while the false\n",
    "    negative rate is assumed to be decreasing.\n",
    "    \"\"\"\n",
    "    axis_min = min(fps[0],fns[-1])\n",
    "    fig,ax = plt.subplots()\n",
    "    plt.plot(fps,fns)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('False Positive Rate (%)')\n",
    "    plt.ylabel('False Negative Rate (%)')\n",
    "    ticks_to_use = [0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,1,2,5,10,20,50]\n",
    "    ax.get_xaxis().set_major_formatter(plt.matplotlib.ticker.ScalarFormatter())\n",
    "    ax.get_yaxis().set_major_formatter(plt.matplotlib.ticker.ScalarFormatter())\n",
    "    ax.set_xticks(ticks_to_use)\n",
    "    ax.set_yticks(ticks_to_use)\n",
    "    plt.axis([0.001,50,0.001,50])\n",
    "    plt.grid(True)\n",
    "\n",
    "DETCurve(fps,fns)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cr2RSJ4WQSE4",
    "5cV-hXFo0Nxa",
    "6t_i7A2IsktS",
    "PksJfQ1JbEIv",
    "A-sZmgmSeOCh",
    "msgsUxHQCL7d",
    "C7WPqyAvVQd6",
    "h0dRXypAMxo2",
    "XFU_0t0dflTg",
    "R8VBaCYBloF2"
   ],
   "name": "training_new_approach.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

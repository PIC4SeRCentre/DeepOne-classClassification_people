{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmhG0pdpbKwt"
   },
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xPn94WQkxH32",
    "outputId": "b7b760d9-3e3c-4166-bc37-f4587dff9679"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from six.moves import urllib\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZHXfZ5IYK8g"
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "preprocess_input = applications.mobilenet_v2.preprocess_input \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run models on GPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[1], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set useful paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a main folder *Dataset* that contains all test images in sub folder *test_IR*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds = \"Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgWpq37tsu1i"
   },
   "source": [
    "## Losses computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two customized losses must be defined to import correctly the trained model\n",
    "\n",
    "Quantities *batch_size* and *sub_batch_size* are defined, with also the constant *beta* which is used in the *compactness loss* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Cu1ytXPl-RXL",
    "outputId": "26cd2b9b-9aa7-423d-e971-5f8109595988"
   },
   "outputs": [],
   "source": [
    "#batch_size = 256   \n",
    "batch_size = 32\n",
    "sub_batch_size = batch_size // 2\n",
    "beta = sub_batch_size**2 / (sub_batch_size-1)**2  #1.1377777777777778\n",
    "print(\"beta = \", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two input quantities of *compactness loss* are:\n",
    "\n",
    "• *y_true*: the true labels of the batch, of size (batch_size, n_classes_ref).\n",
    "This quantity is not used in the lc computation because it has no role in\n",
    "imposing similarity among person features;\n",
    "\n",
    "• *y_pred*: predictions of the intermediate features for each element in the\n",
    "batch, of size (batch_size, n_features). It is produced by the average pooling layer, so the number of features is 1280. We choose this layer because it has weights pre-trained on ImageNet, that speed up the learning process compared to those with random inizialization.\n",
    "\n",
    "In order to consider only features of person images, the first half part of the batch is isolated. Then, the following operations are performed: the variance of the feature distribution along the batch for each feature and the mean of all variances. This number is then multiplied by a correction factor beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VoPxt-FE2Q2U"
   },
   "outputs": [],
   "source": [
    "def compactness_loss(y_true, y_pred):\n",
    "    #y_pred_target = y_pred[0:128]   #shape (128, 2048)\n",
    "    y_pred_target = y_pred[0:16]   #shape (16, 2048)\n",
    "    # ERRATA -> l_c = tf.keras.backend.mean(tf.keras.backend.var(y_pred_target, axis = 1, keepdims=False)) \n",
    "    #axis = 1 means variance along the row -> tf.keras.backend.var of shape=(128,)\n",
    "    l_c = tf.keras.backend.mean(tf.keras.backend.var(y_pred_target, axis = 0, keepdims=False)) \n",
    "    #axis = 0 means variance along the columns (so the features)-> tf.keras.backend.var of shape=(1280,)\n",
    "\n",
    "    return l_c * beta\n",
    "\n",
    "#when features are extraxted from convolutional layer: apply average pooling layer ->  compute loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *descriptiveness loss* is computed using the *cross-entropy loss*, that is here defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tQXtD10Zcpv"
   },
   "outputs": [],
   "source": [
    "#Categorical crossentropy loss used in the descriptiveness loss\n",
    "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False) \n",
    "\n",
    "#**Note - Using from_logits=True is more numerically stable.** -> remove softmax layer\n",
    "#used default redution: reduction=losses_utils.ReductionV2.AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two input quantities of *descriptiveness loss* are:\n",
    "\n",
    "• *y_true*: the true labels of the batch, of size (batch_size, n_classes_ref).\n",
    "This quantity is provided by the inputgenerator, later defined;\n",
    "\n",
    "• *y_pred*: predictions coming from the last fully connected layer, of size\n",
    "(batch_size, n_classes_ref). The second dimension n_classes_ref is 20,\n",
    "corresponding to the categorical label of classes from the reference dataset.\n",
    "The label of the person class is not included because this is not a multiclass classification problem.\n",
    "\n",
    "The descriptiveness loss is computed with respect to only elements of the reference dataset. Therefore, the second half part of the batch is considered both in *y_true* and in *y_pred*. The first part of them contains meaningless numbers, because we don’t care about person image labels.\n",
    "Then, the categorical cross-entropy loss is evaluated between the predicted labels and the desired ones and it is minimized to realize a good classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKNJj57E2Ym6"
   },
   "outputs": [],
   "source": [
    "def descriptiveness_loss(y_true, y_pred): \n",
    "    #y_true_reference = y_true[128:256]  #shape (128, n_classes_ref)\n",
    "    #y_pred_reference = y_pred[128:256]  #shape (128, n_classes_ref)   \n",
    "    y_true_reference = y_true[16:32]  #shape (16, n_classes_ref)\n",
    "    y_pred_reference = y_pred[16:32]  #shape (16, n_classes_ref)     \n",
    "    l_d = cce(y_true_reference, y_pred_reference)\n",
    "    return l_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model and isolate *model_features* for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWbHYVw7mWq_"
   },
   "outputs": [],
   "source": [
    "path_model = os.path.join(path_ds, \"my_model200_400.h5\")  #/content/drive/My Drive/my_model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1Tf4dzRviE5"
   },
   "outputs": [],
   "source": [
    "model_tot = load_model(path_model, custom_objects={'compactness_loss': compactness_loss, 'descriptiveness_loss': descriptiveness_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize properties of all layers that are part of the *model_tot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DK5_yMplxmzp"
   },
   "outputs": [],
   "source": [
    "model_tot.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From *model_tot* we extrapolate the model for feature extraction: *model_features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZzaJBQ0qXz6"
   },
   "outputs": [],
   "source": [
    "model_features = Model(model_tot.inputs, model_tot.layers[-2].output) #output = <tf.Tensor 'dense_2/Identity:0' shape=(None, 1024) dtype=float32>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize properties of all layers from the *model_features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIEbjFmJlPFa"
   },
   "outputs": [],
   "source": [
    "model_features.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPfwNrn45-8Q"
   },
   "source": [
    "# Testing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSmQASU3mm3b"
   },
   "source": [
    "The testing part is realized by a *template matching framework*: firstly, in the *template generation phase*, some baseline features of person intances are stored as templates and then, in *matching phase*, a score is generated considering the Euclidean distance between them and new features from the test image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Templates generation IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In template generation phase, some IR samples are selected and sent into *model_features* to generate *IR templates*.\n",
    "\n",
    "They are in folder *templatesIR224* (already resized into 224x224images) and are 20.\n",
    "\n",
    "*templates* of size (20,1280) are produced by *test_datagen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xD2ML82MEFfo"
   },
   "outputs": [],
   "source": [
    "path_templates_IR = os.path.join(path_ds, \"templatesIR224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(path_templates_IR,\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  shuffle = False,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  batch_size= len(os.listdir(os.path.join(path_templates_IR, \"templates_Persona\"))))  #50\n",
    "\n",
    "templates = model_features.predict(test_generator,steps = 1)  #(n_templates, n_features) ex. (40, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "batch = next(test_generator)\n",
    "for i in range (0,20):\n",
    "    image = batch[0][i]\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0dRXypAMxo2"
   },
   "source": [
    "## 1.1 Creation of  predictions *features_test* and true labels *Y_test* (in DOC labels are 0: target class = Person, 1: alien class, no people inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtUXIQB4gTbL"
   },
   "source": [
    "In this part we extract features from test images. The IR dataset contains 55 pictures with people and 55 pictures without individuals.\n",
    "\n",
    "We take 224x224 IR test images from folder *testIR*:\n",
    "\n",
    "<pre>\n",
    "<b>testIR</b>\n",
    "|__ <b>Persona</b>\n",
    "   |__ <b>0</b>\n",
    "|__ <b>Others</b>\n",
    "   |__ <b>1</b>\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbH9xAyMmX2l"
   },
   "source": [
    "Set test images path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = os.path.join(path_ds, \"testIR\")\n",
    "\n",
    "path_test_Persona = os.path.join(path_test, \"Persona\")\n",
    "\n",
    "path_test_Others = os.path.join(path_test, \"Others\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from Person images using *test_datagen0*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen0 = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "test_generator0 = test_datagen0.flow_from_directory(path_test_Persona,\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  shuffle = False,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  batch_size= 55 \n",
    "                                                  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Persona = model_features.predict(test_generator0, steps = len(os.listdir(os.path.join(path_test_Persona, \"0\"))) // 55) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Persona.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from Others images using *test_datagen1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen1 = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "test_generator1 = test_datagen1.flow_from_directory(path_test_Others,\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  shuffle = False,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  batch_size= 55 \n",
    "                                                  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JiBRZZYujwzy"
   },
   "outputs": [],
   "source": [
    "features_Others = model_features.predict(test_generator1, steps = len(os.listdir(os.path.join(path_test_Others, \"1\"))) // 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Others.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append all features in *features_test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = np.concatenate([features_Persona, features_Others])    #Y_test will be 00000 ... 11111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a *features_test_tsne* for visualizing features and templates thanks to t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_tsne = np.concatenate([features_Persona, features_Others, templates]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create true labels of test images in *Y_test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=np.concatenate([np.zeros(features_Persona.shape[0]), np.ones(features_Others.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a *Y_test_tsne* with inside true labels of features and templates for t-SNE visualization of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_tsne=np.concatenate([np.zeros(features_Persona.shape[0]), np.ones(features_Others.shape[0]), np.ones(templates.shape[0])*2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement t-SNE visualization of 1280 features extracted from each test image.\n",
    "\n",
    "• red points with labels 0 are the features associated to images containg people;\n",
    "\n",
    "• green points labeled with 1 are the features extracted from pictures with no people;\n",
    "\n",
    "• blue points with a fake label 2 are the templates from which the classification score is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "#feat_cols = [ 'pixel'+str(i) for i in range(features_test.shape[1]) ]\n",
    "#df = pd.DataFrame(features_test,columns=feat_cols)\n",
    "#df['y'] = Y_test\n",
    "\n",
    "feat_cols = [ 'pixel'+str(i) for i in range(features_test_tsne.shape[1]) ]\n",
    "df = pd.DataFrame(features_test_tsne,columns=feat_cols)\n",
    "df['y'] = Y_test_tsne\n",
    "\n",
    "df['label'] = df['y'].apply(lambda i: str(i))\n",
    "#features_test, Y_test = None, None\n",
    "print('Size of the dataframe: {}'.format(df.shape))\n",
    "\n",
    "# For reproducability of the results\n",
    "np.random.seed(14)\n",
    "rndperm = np.random.permutation(df.shape[0])\n",
    "\n",
    "N = Y_test.shape[0]\n",
    "df_subset = df.loc[rndperm[:N],:].copy()\n",
    "data_subset = df_subset[feat_cols].values\n",
    "#pca = PCA(n_components=3)\n",
    "#pca_result = pca.fit_transform(data_subset)\n",
    "#df_subset['pca-one'] = pca_result[:,0]\n",
    "#df_subset['pca-two'] = pca_result[:,1] \n",
    "#df_subset['pca-three'] = pca_result[:,2]\n",
    "#print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(data_subset)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 3),\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xEbCv2Blexs"
   },
   "source": [
    "## 2. Template matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In matching phase, features extracted from test images are compared to templates using a *matching function f*, the Euclidean distance.\n",
    "\n",
    "In particular the inputs provided to *scores_generation* function are:\n",
    "\n",
    "• features_test: features extracted from test images, of size (n_test, n_features);\n",
    "\n",
    "• templates: stored templates corresponding to baseline characteristics of\n",
    "the person class, of size (n_templates, n_features).\n",
    "\n",
    "Features coming from each test image are compared to all templates: the quantity *d* contains the euclidean distances between them, sizing (n_templates,). Each vector *d* is computed for all images in the test dataset and is saved in the *distances_vector*, of size (n_test, n_templates). The scores, stored in the vector *scores* of size (n_test,), are selected taking the minimum value among all computed distances in distances_vector, for all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pnHMnmWXjV3j"
   },
   "outputs": [],
   "source": [
    "#def euclidean(v1, v2):\n",
    "#  return sum((p-q)**2 for p, q in zip(v1, v2)) ** .5\n",
    "\n",
    "#d = [euclidean(f, t) for t in templates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlCy6Dejhgmd"
   },
   "outputs": [],
   "source": [
    "def scores_generation(features_test, templates):\n",
    "    for f in features_test:\n",
    "        d = [np.linalg.norm(f-t) for t in templates]  #np.linalg.norm(f-t) = Euclidean norm \n",
    "        distances_vector.append(d)                    #shape of (n_X_test, n_templates): euclidean norm between each element of features_X_test and each template  \n",
    "    scores = np.amin(distances_vector, axis=1)      #axis=1 -> min value for each row\n",
    "    #scores = np.mean(distances_vector, axis=1)\n",
    "    scores = np.array(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YI5I5BrZLmVF"
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "distances_vector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9bKa_7P_hpc"
   },
   "outputs": [],
   "source": [
    "scores = scores_generation(features_test, templates)  #shape of (n_X_test,) for each image belonging to X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y1TUelI5_zla",
    "outputId": "fd933994-ce70-41ef-fd8b-59c64052a98b"
   },
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhrW83kL0tYK"
   },
   "outputs": [],
   "source": [
    "#scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIfSscA3tiru"
   },
   "source": [
    "## Plot ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic curve plots the True Positive Rate (TPR) versus the False Positive Rate (FPR) for all possible thresholds. It is used to evaluate DOC models.\n",
    "Best ones have ROC curves very close to the top left corner of\n",
    "the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_V2Qoj8Ttqm0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "#from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrs7-T5duqta"
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.legend()\n",
    "    plt.title('ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tdE9aA5Fn4X7",
    "outputId": "2db4b15f-c642-4ece-c1f6-6e6072e7992f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(Y_test, scores)\n",
    "AUC = roc_auc_score(Y_test, scores)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oqqifS7e1cJ6",
    "outputId": "b657e425-7949-4a9c-8c4f-9171f8f8dbc3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr, label='DeepOneClassification(AUC = %.2f)'%AUC)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute variance of Person features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the quantity minimized in the compactness loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Persona = tf.convert_to_tensor(features_Persona, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.var(features_Persona, axis = 0, keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var = tf.keras.backend.mean(tf.keras.backend.var(features_Persona, axis = 0, keepdims=False))\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal threshold and DOC output *y_pred*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores are finally transformed in considerable output for One-Class Classification thanks to a threshold delta. Remember that labels in DOC are 0: person and 1:others, so the class person is the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen $\\delta$ in our Deep One-class Classification is the one that maximizes the quantity (TNR-FNR), producing an high value of TNR, the True Negative Rate, and a low value of FNR, the False Negative Rate.\n",
    "The first one indicates the ratio of negative instances correctly classified as negative, while the second one is the ratio of positive instance incorrectly classified.\n",
    "Therefore, maximizing the term (TNR-FNR) allows to reach an high value of intances classified as people that are actually people and a low value of alien objects wrongly classified as people.\n",
    "Considering also that TNR=1-FPR and FNR=1-TPR, finding the maximum value for (TNR-FNR) means maximizing (TPR-FPR) ->TNR-FNR=1-FPR-(1-TPR)=TPR-FPR, that corresponds to the closest point to the top left corner of the ROC curve. These quantities have been already computed by *roc_curve* command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Threshold value is:\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros(scores.shape[0])\n",
    "for i in range(scores.shape[0]):\n",
    "    if scores[i] > optimal_threshold:\n",
    "        y_pred[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are \"reverse\" in DOC (0: person, 1:others) w.r.t. binary models. \n",
    "\n",
    "The fact that the positive class is not the *person class* causes issues because metrics are closely related to the chosen positive class.\n",
    "\n",
    "If we want to refer all metrics that are precision, recall, F1 score, accuracy to the target class, we need to reverse label values produced by DOC models -> 1-Y_test and 1-y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(1-Y_test, 1-y_pred)\n",
    "print(cm)\n",
    "#           predicted               0:negative - others   1:positive - person\n",
    "           #   0  1                   FP=false positive, actual others but predicted person\n",
    "#actual    #0 TN FP                   FN=false negative, actual person but predicted others\n",
    "           #1 FN TP                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(1-Y_test, 1-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(1-Y_test, 1-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(1-Y_test, 1-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "#accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "# or simply\n",
    "accuracy_score(1-Y_test, 1-y_pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cr2RSJ4WQSE4",
    "5cV-hXFo0Nxa",
    "6t_i7A2IsktS",
    "PksJfQ1JbEIv",
    "A-sZmgmSeOCh",
    "msgsUxHQCL7d",
    "C7WPqyAvVQd6",
    "h0dRXypAMxo2",
    "XFU_0t0dflTg",
    "R8VBaCYBloF2"
   ],
   "name": "training_new_approach.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

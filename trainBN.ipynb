{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmhG0pdpbKwt"
   },
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xPn94WQkxH32",
    "outputId": "b7b760d9-3e3c-4166-bc37-f4587dff9679"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from six.moves import urllib\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZHXfZ5IYK8g"
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "preprocess_input = applications.mobilenet_v2.preprocess_input \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run models on GPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')  \n",
    "tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[1], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-ZvjcWjf01T"
   },
   "source": [
    "## Set useful paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder structure is the following: there is a main folder *Dataset* that contains all sub folders where taking picture to pre-process (*path_data*) and where putting all pre-processed ones (*path_train*).\n",
    "\n",
    "The class we want to recognize among all is the *target class Persona*. \n",
    "\n",
    "We will also define, connected to folder *train*, a *path_reference* where inserting all classes from the reference dataset and a *path_target* where inserting the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds = \"Dataset\"\n",
    "path_data = os.path.join(path_ds, \"data\")  #Dataset/data2\n",
    "#path_train = os.path.join(path_ds, \"trainBN\")\n",
    "path_train = os.path.join(path_ds, \"trainBN200\")\n",
    "if not os.path.exists(path_train):\n",
    "      os.makedirs(path_train)\n",
    "target = \"Persona\"\n",
    "\n",
    "path_reference = os.path.join(path_train, \"reference\")  #Dataset/train2/reference\n",
    "if not os.path.exists(path_reference):\n",
    "      os.makedirs(path_reference)\n",
    "path_target = os.path.join(path_train, \"target\")        #Dataset/train2/target\n",
    "if not os.path.exists(path_target):\n",
    "      os.makedirs(path_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print how many people images we have to pre-process and also how many classes of the reference dataset are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pri1OqimNWI9"
   },
   "outputs": [],
   "source": [
    "print(\"There are \", len(os.listdir(os.path.join(path_data, target))), \" people images to process inside the target dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes_ref = len(os.listdir(path_data))-1\n",
    "print(\"There are \" + str(n_classes_ref) + \" classes belonging to the reference dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4Em_ryk6Eqw"
   },
   "source": [
    "# Training part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cr2RSJ4WQSE4"
   },
   "source": [
    "\n",
    "## Pre-process images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "imZyuFcAf0i0"
   },
   "source": [
    "The used *reference dataset* is a subset of the training set of ILSVR2012. Our set is composed of randomly chosen 20 classes (from the original 1000), each containing 500 images, for a total of 10 thousand of images.\n",
    "\n",
    "Since ILSVR2012 is organized in *synsets*, multiple words or phrases that describe a meaningful concept, we create the *synset_to_human* dictionary to map synsets to human-readable names. This helps in creating the correct folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1cMsQ_Nf01N"
   },
   "outputs": [],
   "source": [
    "def create_readable_names_for_imagenet_labels():\n",
    "    \"\"\"Create a dict mapping label id to human readable string.\n",
    "    Returns:\n",
    "            synset_to_human: dictionary mapping synsets and human-readable names.\n",
    "\n",
    "    'imagenet_lsvrc_2015_synsets.txt' contains a list of valid synset labels used by ILSVRC competition.\n",
    "    There is one synset one per line, eg.\n",
    "              #   n01440764\n",
    "              #   n01443537\n",
    "    'imagenet_metadata.txt' contains a mapping from synsets to human-readable names for every synset in Imagenet. \n",
    "    These are stored in a tsv format, as follows:\n",
    "              #   n02119247    black fox\n",
    "              #   n02119359    silver fox\n",
    "\n",
    "    Code is based on\n",
    "    https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py#L463\n",
    "    \"\"\"\n",
    "    filename = 'imagenet_lsvrc_2015_synsets.txt'\n",
    "    synset_list = [s.strip() for s in open(filename).readlines()]\n",
    "    num_synsets_in_ilsvrc = len(synset_list)\n",
    "    assert num_synsets_in_ilsvrc == 1000\n",
    "\n",
    "    filename = 'imagenet_metadata.txt'\n",
    "    synset_to_human_list = open(filename).readlines()\n",
    "    num_synsets_in_all_imagenet = len(synset_to_human_list)\n",
    "    assert num_synsets_in_all_imagenet == 21842\n",
    "\n",
    "    synset_to_human = {}\n",
    "    for s in synset_to_human_list:\n",
    "        parts = s.strip().split('\\t')\n",
    "        assert len(parts) == 2\n",
    "        synset = parts[0]\n",
    "        human = parts[1]\n",
    "        if synset in synset_list:\n",
    "            synset_to_human[synset] = human\n",
    "\n",
    "    return synset_to_human\n",
    "\n",
    "synset_to_human = create_readable_names_for_imagenet_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "CVKR1_aAf01Q",
    "outputId": "ec580d93-f994-4d34-b7fe-768dcf5fa830"
   },
   "outputs": [],
   "source": [
    "print(len(synset_to_human))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XS5rw9oQPf0K"
   },
   "source": [
    "In the pre-processing part for the training, we take pictures from *data* which is composed of:\n",
    "<pre>\n",
    "<b>data</b>\n",
    "|__ <b>n01443537</b>\n",
    "|__ <b>n01484850</b>\n",
    "|__ <b>n01532829</b>\n",
    "|__ <b>n01882714</b>\n",
    "|__ <b>n--------</b>\n",
    "|__ <b>n--------</b>\n",
    "|__ <b>Persona</b>\n",
    "</pre>\n",
    "\n",
    "and we put the pre-processed images into *trainBN*, realizing this structure:\n",
    "\n",
    "<pre>\n",
    "<b>trainBN</b>\n",
    "|__ <b>target</b>\n",
    "    |__ <b>Persona</b>\n",
    "|__ <b>reference</b>\n",
    "    |__ <b>acoustic guitar</b>\n",
    "    |__ <b>African elephant, Loxodonta africana</b>\n",
    "    |__ <b>analog clock</b>\n",
    "    |__ <b>backpack, back pack, knapsack, packsack, rucksack, haversack</b>\n",
    "    |__ <b>beer glass</b>\n",
    "    |__ --------   \n",
    "</pre>\n",
    "\n",
    "All pictures in *data* are color (RGB) images, but the training is carried out using grayscale images. This is why a future goal of this discussion is to extend the Deep One-class Classification in InfraRed images, in order to recognize people in frames coming from surveillance videos, even at night.\n",
    "Problems in training networks directly with InfraRed examples arise because\n",
    "there are no large datasets made of enough images like ImageNet dataset or\n",
    "Open Images Dataset. That’s why a training with grayscale pictures is proposed and, then, a generalization to IR datasets through a further transfer learning is suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB datasets are transformed to grayscale datasets using the library *OpenCV*. Steps in the pre-processing part are:\n",
    "\n",
    "• each image is centrally cropped along its smaller size. In this way we\n",
    "can resize it without altering the image aspect ratio and the properties\n",
    "of objects within;\n",
    "\n",
    "• each picture is resized to square format of 224×224 with a bilinear interpolation;\n",
    "\n",
    "• each image is made a grayscale image with size of (224, 224, 1), having a\n",
    "single channel;\n",
    "\n",
    "• each grayscale image is brought back on three channels, repeating the single channel three times. This operation is done since the structure of\n",
    "most of networks presents a three channel configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "for folder in os.listdir(path_data):\n",
    "    path_folder = os.path.join(path_data, folder)\n",
    "    print(\"\\n------------------------------------------------------\")\n",
    "    print(\"\\nFolder \", folder, \" with \", len(os.listdir(path_folder)), \"images inside\")\n",
    "\n",
    "    if folder == target:\n",
    "        path_out = os.path.join(path_target, folder)\n",
    "        if not os.path.exists(path_out):\n",
    "            os.makedirs(path_out)\n",
    "    else: \n",
    "        path_out = os.path.join(path_reference, synset_to_human[folder])\n",
    "        if not os.path.exists(path_out):\n",
    "            os.makedirs(path_out)\n",
    "\n",
    "    i=0   #new images\n",
    "    j=0   #images already pre-processed\n",
    "    for file in os.listdir(path_folder):\n",
    "        if os.path.exists(path_out + \"/\" + file):\n",
    "            j+=1\n",
    "            print(\"Image \" + file + \" already pre-processed\" )\n",
    "        else:\n",
    "            i+=1\n",
    "            print(\"Processing ... \", file)\n",
    "            \n",
    "            #read the image\n",
    "            image = cv2.imread(path_folder + \"/\"+ file)\n",
    "            #crop image -> square image along its min dimension\n",
    "            h, w, c = image.shape\n",
    "            if w>h:\n",
    "                start = (w-h)//2\n",
    "                image = image[:, start:start+h]\n",
    "            else:\n",
    "                start = (h-w)//2\n",
    "                image = image[start:start+w,:]\n",
    "            #resize\n",
    "            image = cv2.resize(image, (img_size, img_size), interpolation=cv2.INTER_LINEAR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)     #gray image\n",
    "            image = cv2.merge((image, image, image))            #gray image on 3 channels\n",
    "            #write the pre-proc image in train folder\n",
    "            cv2.imwrite(path_out + \"/\" + file, image)\n",
    "\n",
    "    print(\"\\nImages that have been previously pre-processed: \" + str(j))\n",
    "    print(\"\\nNewly pre-processed images: \" + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xhSVVJM4NIf"
   },
   "source": [
    "Classes of the reference dataset are therefore printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "6ydiKiy-wJ6B",
    "outputId": "72ee894b-6cb9-48f1-d01c-581608180021"
   },
   "outputs": [],
   "source": [
    "classes_ref = []  #list\n",
    "classes_ref = [name for name in os.listdir(path_reference)]\n",
    "print(\"Classes of reference dataset are \", classes_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cV-hXFo0Nxa"
   },
   "source": [
    "## Build the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a unique MobileNetV2 in *base_model*. MobileNetV2 by Google belongs to MobileNets family, efficient and optimized architectures for mobile devices. It is fast and provides high accuracy, requiring few parameters and low computational power, also compared to previous versions.\n",
    "\n",
    "• We load *weights* pre-trained on ImageNet, without including the default top part with 1000 neurons;\n",
    "\n",
    "• The *input shape* of images is set to (224, 224, 3);\n",
    "\n",
    "• The hyperparameter alpha, belonging to range (0, 1] and known as the width multiplier that determines the number of filters at each layer, is set to its default value 1;\n",
    "\n",
    "• A *global average pooling layer* is inserted after the the last convolutional block, passing from a 4D output tensor of shape (batch_size, 7, 7, 1280) to a flattened 2D output tensor of shape (batch_size, 1280). These 1280 numbers are the features extracted from the input images, from which we minimize the compactness loss and on which we base the classification of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = len(os.listdir(path_data))-1\n",
    "alpha = 1.0 #for MobileNetV2\n",
    "base_model = applications.MobileNetV2(include_top=False, \n",
    "                                      input_shape=(224, 224, 3), \n",
    "                                      alpha=alpha, \n",
    "                                      weights='imagenet',\n",
    "                                      pooling=\"avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• A *fully connected layer* with a softmax activation function and with\n",
    "a number of units equal to the total classes of the reference dataset (20) is\n",
    "attached, in order to compute the descriptiveness loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Dense(classes, activation='softmax')(base_model.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras model is defined, having:\n",
    "\n",
    "• 1 input, the base model input that is an image batch of size (batch_size, 224, 224, 3);\n",
    "\n",
    "• 2 outputs, that are the *global average pooling layer output*, representing the extracted features from MobileNetV2 and the *fully connected layer output*, representing the classification predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We extract features from the average pooling layer to take advantages from loaded pre-trained weights. Layer predictions is not pre-trained!\n",
    "model = keras.Model(inputs=base_model.input,outputs=[base_model.get_layer(\"global_average_pooling2d\").output, predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EarxZnumwR80"
   },
   "outputs": [],
   "source": [
    "#Seconda versione con \n",
    "#FC1 = Dense(540, activation='relu')(base_model.output) #540 neurons because we don't want sharp variation from 1280 to n_classes_ref\n",
    "#FC2 = Dense(200, activation='relu')(FC1)              #we can add other layers later\n",
    "#predictions = Dense(n_classes_ref, activation = 'softmax')(FC2)\n",
    "\n",
    "#model = keras.Model(inputs=base_model.input,outputs=[base_model.get_layer(\"global_average_pooling2d\").output, predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the number of features extracted from each single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "827HM2sIRyTk",
    "outputId": "a3db0c9a-a3b4-4aba-fa71-23281d08c359"
   },
   "outputs": [],
   "source": [
    "n_features = base_model.get_layer(\"global_average_pooling2d\").output.shape[1]\n",
    "print(n_features)   #1280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.output) #shape=(None, 1280) shape=(None, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize properties of all layers that are part of the model and their number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OfUwoNHy0ZM8",
    "outputId": "bf0866fc-80a9-4873-8499-baad2e77e8f4"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of layers in the model: \", len(model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training phase some layers of the network are *frozen*, to preserve\n",
    "imported parameters pre-trained on ImageNet. This means we take low-level\n",
    "features learned in a different classification task, by leveraging them\n",
    "in our problem.\n",
    "\n",
    "In MobileNetV2 we choose to initially freeze all blocks until block 13, having 40 unfrozen layers over the whole 157 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed weights\n",
    "for layer in model.layers:\n",
    "    #if layer.name == \"Conv_1\":\n",
    "    if layer.name == \"block_13_expand\":\n",
    "        break\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GC9XJnLKCRo"
   },
   "outputs": [],
   "source": [
    "#model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4NKThhJKJwL1",
    "outputId": "95d33389-f92a-4660-c611-2c646310f4f2"
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "for layer in model.layers:\n",
    "    #print(layer, layer.trainable)\n",
    "    if layer.trainable == True:\n",
    "        k=k+1\n",
    "print(\"Layers with trainable=True: \", k, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgWpq37tsu1i"
   },
   "source": [
    "## Losses computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is fed with a big input batch of size *batch_size*, that is composed of two smaller batches of the same size *sub_batch_size*, called *target sub-batch* and the *reference sub-batch*.\n",
    "\n",
    "Quantities *batch_size* and *sub_batch_size* are defined, with also the constant *beta* which is used in the *compactness loss* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Cu1ytXPl-RXL",
    "outputId": "26cd2b9b-9aa7-423d-e971-5f8109595988"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sub_batch_size = batch_size // 2\n",
    "beta = sub_batch_size**2 / (sub_batch_size-1)**2  #1.0158 with batch_size=256\n",
    "print(\"beta = \", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *compactness loss* is aimed at minimizing the variance of features of each batch and is computed exclusively considering objects from the target dataset, that are pictures with people inside.\n",
    "\n",
    "The two input quantities are:\n",
    "\n",
    "• *y_true*: the true labels of the batch, of size (batch_size, n_classes_ref).\n",
    "This quantity is not used in the lc computation because it has no role in\n",
    "imposing similarity among person features;\n",
    "\n",
    "• *y_pred*: predictions of the intermediate features for each element in the\n",
    "batch, of size (batch_size, n_features). It is produced by the average pooling layer, so the number of features is 1280. We choose this layer because it has weights pre-trained on ImageNet, that speed up the learning process compared to those with random inizialization.\n",
    "\n",
    "In order to consider only features of person images, the first half part of the batch is isolated. Then, the following operations are performed: the variance of the feature distribution along the batch for each feature and the mean of all variances. This number is then multiplied by a correction factor beta.\n",
    "\n",
    "Minimizing the mean of the variance of all the features implies having similar\n",
    "characteristics for all images representing people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VoPxt-FE2Q2U"
   },
   "outputs": [],
   "source": [
    "def compactness_loss(y_true, y_pred):\n",
    "    #y_pred_target = y_pred[0:128]   #shape (128, 1280)\n",
    "    y_pred_target = y_pred[0:16]   #shape (16, 1280)\n",
    "    # ERRATA -> l_c = tf.keras.backend.mean(tf.keras.backend.var(y_pred_target, axis = 1, keepdims=False)) \n",
    "    #axis = 1 means variance along the row -> tf.keras.backend.var of shape=(128,)\n",
    "    l_c = tf.keras.backend.mean(tf.keras.backend.var(y_pred_target, axis = 0, keepdims=False)) \n",
    "    #axis = 0 means variance along the columns (so the features)-> tf.keras.backend.var of shape=(2048,)\n",
    "\n",
    "    return l_c * beta\n",
    "\n",
    "#when features are extraxted from convolutional layer: apply average pooling layer ->  compute loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *descriptiveness loss* is computed to have high accuracy in classification and is evaluated considering instances coming only from the reference dataset. It uses the *cross-entropy loss* to state the descriptiveness of features, that is defined at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tQXtD10Zcpv"
   },
   "outputs": [],
   "source": [
    "#Categorical crossentropy loss used in the descriptiveness loss\n",
    "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False) \n",
    "\n",
    "#**Note - Using from_logits=True is more numerically stable.** -> remove softmax layer\n",
    "#used default redution: reduction=losses_utils.ReductionV2.AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two input quantities are:\n",
    "\n",
    "• *y_true*: the true labels of the batch, of size (batch_size, n_classes_ref).\n",
    "This quantity is provided by the inputgenerator, later defined;\n",
    "\n",
    "• *y_pred*: predictions coming from the last fully connected layer, of size\n",
    "(batch_size, n_classes_ref). The second dimension n_classes_ref is 20,\n",
    "corresponding to the categorical label of classes from the reference dataset.\n",
    "The label of the person class is not included because this is not a multiclass classification problem.\n",
    "\n",
    "The descriptiveness loss is computed with respect to only elements of the reference dataset. Therefore, the second half part of the batch is considered both in *y_true* and in *y_pred*. The first part of them contains meaningless numbers, because we don’t care about person image labels.\n",
    "Then, the categorical cross-entropy loss is evaluated between the predicted labels and the desired ones and it is minimized to realize a good classification.\n",
    "\n",
    "In this way, features are characterized by the property of descriptiveness, in addition to compactness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKNJj57E2Ym6"
   },
   "outputs": [],
   "source": [
    "def descriptiveness_loss(y_true, y_pred): \n",
    "    #y_true_reference = y_true[128:256]  #shape (128, 20)\n",
    "    #y_pred_reference = y_pred[128:256]  #shape (128, 20)\n",
    "    y_true_reference = y_true[16:32]  #shape (16, 20)\n",
    "    y_pred_reference = y_pred[16:32]  #shape (16, 20)\n",
    "    l_d = cce(y_true_reference, y_pred_reference)\n",
    "    return l_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is compiled defining:\n",
    "\n",
    "• the *optimizer* as the gradient descent algorithm, employed with a very\n",
    "low learning rate lr = 0.00005 and a weight decay of 0.00005;\n",
    "\n",
    "• the two losses and related weights controlled by lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rYG2YtpFR52L"
   },
   "outputs": [],
   "source": [
    "lambd = 10\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.00005, decay=0.00005), #, momentum=0.9),  #lr=0.2, momentum=0.9, decay=0.01\n",
    "    #optimizer=tf.keras.optimizers.Adam( learning_rate=0.00005,\n",
    "    #                                    beta_1=0.9,\n",
    "    #                                    beta_2=0.999,\n",
    "    #                                    epsilon=1e-07,\n",
    "    #                                    amsgrad=False,\n",
    "    #                                    name=\"Adam\"),\n",
    "    loss=[compactness_loss, descriptiveness_loss],\n",
    "    #loss={\"FC1\": compactness_loss,\n",
    "    #      \"predictions\": descriptiveness_loss},\n",
    "    \n",
    "    loss_weights = [lambd, 1],\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fO-yyonln9D2"
   },
   "source": [
    "## Create a multiple ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a function with multiple Keras ImageDataGenerator objects to handle\n",
    "the training with a big input batch, composed of two smaller batches that are the *target sub-batch* and the *reference sub-batch*.\n",
    "\n",
    "In this way all images are provided to the network \"on the fly\", without storing\n",
    "all matrices in memory and causing related memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbXVTunF2kLH"
   },
   "outputs": [],
   "source": [
    "# the output of inputgenerator is a tuple : ((batch_size, 224, 224, 3), (batch_size, 20))\n",
    "\n",
    "input_imgen = ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "def generate_generator_multiple(generator, dir1, dir2, sub_batch_size, img_height, img_width, n_classes):\n",
    "    genX1 = generator.flow_from_directory(dir1,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = sub_batch_size,\n",
    "                                          shuffle=True)\n",
    "    \n",
    "    genX2 = generator.flow_from_directory(dir2,\n",
    "                                          target_size = (img_height,img_width),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = sub_batch_size,\n",
    "                                          shuffle=True)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            yield np.concatenate([X1i[0], X2i[0]]), np.concatenate([to_categorical(np.argmax(X1i[1], axis=1), num_classes=n_classes_ref), X2i[1]]) \n",
    "            #Yield 2 concatenated batches and their categorical concatenated labels\n",
    "            \n",
    "inputgenerator = generate_generator_multiple(generator = input_imgen,\n",
    "                                             dir1 = path_target,\n",
    "                                             dir2 = path_reference,\n",
    "                                             sub_batch_size = sub_batch_size,\n",
    "                                             img_height = 224,\n",
    "                                             img_width = 224,\n",
    "                                             n_classes = n_classes_ref)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs are delineated from the size of the target dataset.\n",
    "\n",
    "The number of epochs is set to 400, taking care to save intermediate models every 50 epochs to properly study the evolution of tested metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJpo52KIiERr"
   },
   "outputs": [],
   "source": [
    "train_size = len(os.listdir(os.path.join(path_target, target))) #6000\n",
    "epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x2P9Y4ycp7xD",
    "outputId": "f8457179-bdfd-4b72-89a5-6f7223000ad8"
   },
   "outputs": [],
   "source": [
    "print(train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZfuUYe8vKVOD"
   },
   "source": [
    "## Train with *fit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "W5Iy71hdohG6",
    "outputId": "afeaef6f-37ac-43ff-f3b6-a3047bce8fe5"
   },
   "outputs": [],
   "source": [
    "history = model.fit(inputgenerator,\n",
    "                    epochs = epochs,\n",
    "                    steps_per_epoch = train_size // sub_batch_size,\n",
    "                    #use_multiprocessing=True,\n",
    "                    #shuffle=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "vmo0848ApZul",
    "outputId": "c6a87a02-5f50-4045-9e53-d03bd5d1a7d8"
   },
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfwq2QV1HWoE"
   },
   "source": [
    "Retrive losses and accuracy from history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "tw9b2A7kpfjs",
    "outputId": "54310e5f-49f5-4e0d-9f1b-51b3143dc7d9"
   },
   "outputs": [],
   "source": [
    "# Retrieve losses and accuracy\n",
    "total_loss = history.history['loss']\n",
    "l_c = history.history['avg_pool_loss']\n",
    "l_d = history.history['dense_loss']\n",
    "acc_dense = history.history['dense_accuracy']\n",
    "\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(total_loss))\n",
    "\n",
    "print(\"Total loss = \", total_loss)\n",
    "print(\"Compactness loss = \", l_c)\n",
    "print(\"Descriptiveness loss = \", l_d)\n",
    "print(\"Accuracy (dense) = \", acc_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IG-97lPXJx86"
   },
   "source": [
    "## Train with *train_on_batch* (suggested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, values of losses are stored every 10 batch iterations, in order to understand what happens during each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lXTCsB4Wb0H"
   },
   "outputs": [],
   "source": [
    "total_loss=total_loss\n",
    "l_c=l_c\n",
    "l_d=l_d\n",
    "acc=acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss=[]\n",
    "l_c=[]\n",
    "l_d=[]\n",
    "acc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MyAkXmR8-Bam",
    "outputId": "e9cca722-9caa-4f27-cf7d-545051228804"
   },
   "outputs": [],
   "source": [
    "n_batches = train_size // sub_batch_size\n",
    "print(\"Number of batches : \", n_batches)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nEpoch \", epoch+1 , \"/\", epochs)\n",
    "  \n",
    "    for i in range(n_batches):\n",
    "        print(\"Processing batch...  \", i)\n",
    "        batch = next(inputgenerator)\n",
    "        #print(type(batch), batch[0].shape, batch[1].shape)\n",
    "        loss, compactness_loss, descriptness_loss, dense_accuracy, dense_1_accuracy = model.train_on_batch(batch[0], batch[1])\n",
    "        #Print the total loss every 10 iterations\n",
    "        if i % 10 == 0:\n",
    "            print(\"\\nTotal loss after iteration \", i, \" is \", loss)\n",
    "            print(\"\\nCompact loss after iteration \", i, \" is \", compactness_loss)\n",
    "            print(\"\\nDescript loss after iteration \", i, \" is \", descriptness_loss)\n",
    "            total_loss.append(loss)\n",
    "            l_c.append(compactness_loss)\n",
    "            l_d.append(descriptness_loss)\n",
    "            acc.append(dense_1_accuracy)\n",
    "            \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        my_model = \"my_model_l400_\"+ str(epoch+1) +\".h5\"\n",
    "        path_model = os.path.join(path_ds, my_model)  #/content/drive/My Drive/my_model.h5\n",
    "        model.save(path_model)\n",
    "\n",
    "    print(\"Total loss at the end of epoch \" , epoch+1, \": \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPfNG2afDVw5"
   },
   "source": [
    "## Retrieve losses stored in folder *Dataset* if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kye2EVaQkYKK"
   },
   "outputs": [],
   "source": [
    "path_loss = os.path.join(path_ds, \"loss19.json\")\n",
    "path_lc = os.path.join(path_ds, \"l_c19.json\")\n",
    "path_ld = os.path.join(path_ds, \"l_d19.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygJ3x3ZtC5E2"
   },
   "outputs": [],
   "source": [
    "with open(path_loss, 'r') as fp:\n",
    "    total_loss = json.load(fp)\n",
    "with open(path_lc, 'r') as fp:\n",
    "    l_c = json.load(fp)\n",
    "with open(path_ld, 'r') as fp:\n",
    "    l_d = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ADu3kHEuCDI3"
   },
   "source": [
    "## Plot compacteness, descriptiveness and total losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "7RxdBw2fbpbV",
    "outputId": "65fa9c04-b6ee-4311-b7db-464a2f81b070"
   },
   "outputs": [],
   "source": [
    "print(np.mean(total_loss))\n",
    "print(np.mean(l_c))\n",
    "print(np.mean(l_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "TVVII35ZHJab",
    "outputId": "a5456385-224a-42a6-87c9-b7d7e32d9248"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_loss, label=\"Total loss\")\n",
    "plt.plot(l_c, label=\"Compacteness loss\")\n",
    "plt.plot(l_d, label=\"Descriptiveness loss\")\n",
    "plt.xlabel(\"training steps\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "K4fKOmOYrHG_",
    "outputId": "37e236ad-0ad4-4931-d4ce-9a660b3f35db"
   },
   "outputs": [],
   "source": [
    "plt.plot(total_loss, label=\"Total loss\")\n",
    "plt.xlabel(\"training steps\")\n",
    "plt.legend()\n",
    "#plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot compacteness loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "sTbODn2SrLJw",
    "outputId": "d480229f-89c3-4afe-d21d-0a11c408f4a2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(l_c, label=\"Compacteness loss\")\n",
    "plt.xlabel(\"training steps\")\n",
    "#plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot descriptiveness loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "3KlzgGBfrOem",
    "outputId": "4aa6da7e-6c00-4aeb-fd06-c56cdba94a45",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(l_d, label=\"Descriptiveness loss\")\n",
    "plt.xlabel(\"training steps\")\n",
    "#plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot dense accuracy (maximum value is 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc, label=\"Dense accuracy\")\n",
    "plt.xlabel(\"training steps\")\n",
    "#plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "msgsUxHQCL7d"
   },
   "source": [
    "## Save losses on folder *Dataset* if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aeEPVeMhvy0B"
   },
   "outputs": [],
   "source": [
    "path_loss = os.path.join(path_ds, \"loss20.json\") \n",
    "path_lc = os.path.join(path_ds, \"l_c20.json\")\n",
    "path_ld = os.path.join(path_ds, \"l_d20.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9UvdqTGwDcb"
   },
   "outputs": [],
   "source": [
    "with open(path_loss, 'w') as fp:\n",
    "  json.dump(total_loss, fp)\n",
    "with open(path_lc, 'w') as fp:\n",
    "  json.dump(l_c, fp)\n",
    "with open(path_ld, 'w') as fp:\n",
    "  json.dump(l_d, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bt2fyXNzrnvQ"
   },
   "source": [
    "## Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfhgC_h1rpa3"
   },
   "outputs": [],
   "source": [
    "path_model = os.path.join(path_ds, \"my_model_400.h5\")  #/content/drive/My Drive/my_model.h5\n",
    "model.save(path_model)  # creates a HDF5 file 'my_model.h5'\n",
    "#del model  # deletes the existing model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cr2RSJ4WQSE4",
    "5cV-hXFo0Nxa",
    "6t_i7A2IsktS",
    "PksJfQ1JbEIv",
    "A-sZmgmSeOCh",
    "msgsUxHQCL7d",
    "C7WPqyAvVQd6",
    "h0dRXypAMxo2",
    "XFU_0t0dflTg",
    "R8VBaCYBloF2"
   ],
   "name": "training_new_approach.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
